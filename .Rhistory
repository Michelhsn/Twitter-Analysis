library(wordcloud2)
library(tm)
library(RColorBrewer)
library(cluster)
library(fpc)
library(dplyr)
library(tidyr)
library(tidytext)
library(widyr)
library(igraph)
library(ggraph)
# load twitter library - the rtweet library is recommended now over twitteR
library(rtweet)
# plotting and pipes - tidyverse!
library(ggplot2)
library(dplyr)
# text mining library
library(tidytext)
## R Markdown
library(rtweet)
library(wordcloud)
library(wordcloud2)
library(tm)
library(RColorBrewer)
library(cluster)
library(fpc)
library(dplyr)
library(tidyr)
library(tidytext)
library(widyr)
library(igraph)
library(ggraph)
cleaned_tweet_words %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
# geom_edge_link(aes(edge_alpha = n, edge_width = n))
# geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
geom_node_point(color = "darkslategray4", size = 3) +
geom_node_text(aes(label = name), vjust = 1.8, size = 3) +
labs(title = "Word Network: Tweets using the hashtag - Climate Change",
subtitle = "Text mining twitter data ",
x = "", y = "")
tweets_paired_words %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
# geom_edge_link(aes(edge_alpha = n, edge_width = n))
# geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
geom_node_point(color = "darkslategray4", size = 3) +
geom_node_text(aes(label = name), vjust = 1.8, size = 3) +
labs(title = "Word Network: Tweets using the hashtag - Climate Change",
subtitle = "Text mining twitter data ",
x = "", y = "")
tweets_separated_words <- tweets_paired_words %>%
separate(paired_words, c("word1", "word2"), sep = " ")
tweets_filtered <- tweets_separated_words %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
# new bigram counts:
words_counts <- tweets_filtered %>%
count(word1, word2, sort = TRUE)
words_counts %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
# geom_edge_link(aes(edge_alpha = n, edge_width = n))
# geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
geom_node_point(color = "darkslategray4", size = 3) +
geom_node_text(aes(label = name), vjust = 1.8, size = 3) +
labs(title = "Word Network: Tweets using the hashtag - Climate Change",
subtitle = "Text mining twitter data ",
x = "", y = "")
words_counts %>%
filter(n >= 24) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
# geom_edge_link(aes(edge_alpha = n, edge_width = n))
# geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
geom_node_point(color = "darkslategray4", size = 3) +
geom_node_text(aes(label = name), vjust = 1.8, size = 3) +
labs(title = "Word Network: Tweets using the hashtag - Climate Change",
subtitle = "Text mining twitter data ",
x = "", y = "")
# plot the top 15 words -- notice any issues?
tweets_clean %>%
count(word, sort = TRUE) %>%
top_n(15) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y = n)) +
geom_col() +
xlab(NULL) +
coord_flip() +
labs(x = "Count",
y = "Unique words",
title = "Count of unique words found in tweets")
cleaned_tweet_words %>%
count(word, sort = TRUE) %>%
top_n(15) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(x = word, y = n)) +
geom_col() +
xlab(NULL) +
coord_flip() +
labs(y = "Count",
x = "Unique words",
title = "Count of unique words found in tweets",
subtitle = "Stop words removed from the list")
# load twitter library - the rtweet library is recommended now over twitteR
library(rtweet)
# plotting and pipes - tidyverse!
library(ggplot2)
library(dplyr)
# text mining library
library(tidytext)
## R Markdown
library(rtweet)
library(wordcloud)
library(wordcloud2)
library(tm)
library(RColorBrewer)
library(cluster)
library(fpc)
library(dplyr)
library(tidyr)
library(tidytext)
library(widyr)
library(igraph)
library(ggraph)
library(rjson)
library(jsonlite)
# plotting and pipes - tidyverse!
library(ggplot2)
library(dplyr)
library(tidyr)
# animated maps
# to install: devtools::install_github("dgrtwo/gganimate")
# note this required imagemagick to be installed
library(leaflet)
library(gganimate)
library(lubridate)
library(maps)
library(ggthemes)
options(stringsAsFactors = FALSE)
# create file path
json_file <- "data/week-13/boulder_flood_geolocated_tweets.json"
# import json file line by line to avoid syntax errors
# this takes a few seconds
boulder_flood_tweets <- stream_in(file(json_file))
getwd()
# create file path
json_file <- "C:/Users/miche/Documents/Twitter Analysis/boulder_flood_geolocated_tweets.json"
# import json file line by line to avoid syntax errors
# this takes a few seconds
boulder_flood_tweets <- stream_in(file(json_file))
json_file
# import json file line by line to avoid syntax errors
# this takes a few seconds
boulder_flood_tweets <- stream_in(file(json_file))
class(json_file)
cleaned_tweet_words
tweets
# create new df with just the tweet texts & usernames
tweet_data <- data.frame(date_time = tweets$created_at,
username = tweets$user$screen_name,
tweet_text = tweets$text,
coords = tweets$coordinates)
view(tweets)
tweets
# create new df with just the tweet texts & usernames
tweet_data <- data.frame(date_time = tweets$created_at,
username = tweets$screen_name,
tweet_text = tweets$text,
coords = tweets$coords_coords)
tweet_data
tweets$coords_coords
View(twwts)
View(tweets)
tweets$bbox_coords
tweets$geo_coords
tweets$source
tweets$symbols
tweets$place_url
tweets$place_name
tweets$place_full_name
tweets$place_type
tweets$location
tweets$country
tweets$country_code
tweets$geo_coords
tweets$coords_coords
tweets$verified
# what users are tweeting with #rstats
tweets <- search_tweets("#impeachmentRodrigoMaia",
n = 6000)
tweets %>%
count(location, sort = TRUE) %>%
mutate(location = reorder(location,n)) %>%
na.omit() %>%
top_n(20) %>%
ggplot(aes(x = location,y = n)) +
geom_col() +
coord_flip() +
labs(x = "Location",
y = "Count",
title = "Twitter users - unique locations ")
install.packages(c("gganimate", "rjson"))
tweet_data <- data.frame(date_time = tweets$created_at,
username = tweets$screen_name,
tweet_text = tweets$text,
coords = tweets$coords_coords)
tweet_data <- data.frame(date_time = tweets$created_at,
username = tweets$screen_name,
tweet_text = tweets$text,
coords = tweets$text)
tweet_data <- data.frame(date_time = tweets$created_at,
username = tweets$screen_name,
tweet_text = tweets$text,
coords = tweets$coords_coords)
tweet_data <- data.frame(date_time = tweets$created_at,
username = tweets$screen_name,
tweet_text = tweets$text,
coords = tweets$geo_coords)
tweets$geo_coords
tweets$geo_coords[0]
tweets$geo_coords[1]
tweets$geo_coords[1][1]
tweets$geo_coords[1][1][1]
as.character(tweets$geo_coords)
tweet_data <- data.frame(date_time = tweets$created_at,
# flood start date sept 13 - 24 (end of incident)
start_date <- as.POSIXct('2013-09-13 00:00:00')
end_date <- as.POSIXct('2013-09-24 00:00:00')
end_date <- as.POSIXct('2013-09-24 00:00:00')
# flood start date sept 13 - 24 (end of incident)
start_date <- as.POSIXct('2013-09-13 00:00:00')
# cleanup & and filter to just the time period around the flood
flood_tweets <- tweet_data %>%
mutate(coords.coordinates = gsub("\\)|c\\(", "", coords.coordinates),
date_time = as.POSIXct(date_time, format = "%a %b %d %H:%M:%S +0000 %Y")) %>%
separate(coords.coordinates, c("long", "lat"), sep = ", ") %>%
mutate_at(c("lat", "long"), as.numeric) %>%
filter(date_time >= start_date & date_time <= end_date )
# cleanup & and filter to just the time period around the flood
flood_tweets <- tweet_data %>%
mutate(coords = gsub("\\)|c\\(", "", coords),
date_time = as.POSIXct(date_time, format = "%a %b %d %H:%M:%S +0000 %Y")) %>%
separate(coords, c("long", "lat"), sep = ", ") %>%
mutate_at(c("lat", "long"), as.numeric) %>%
filter(date_time >= start_date & date_time <= end_date )
flood_tweets
world_basemap <- ggplot() +
borders("world", colour = "gray85", fill = "gray80")
world_basemap
world_basemap <- ggplot() +
borders("world", colour = "gray85", fill = "gray80") +
theme_map()
world_basemap
# remove na values
tweet_locations <- flood_tweets %>%
na.omit()
world_basemap +
geom_point(data = tweet_locations, aes(x = long, y = lat),
colour = 'purple', alpha = .5) +
scale_size_continuous(range = c(1, 8),
breaks = c(250, 500, 750, 1000)) +
labs(title = "Tweet Locations During the Boulder Flood Event")
site_locations <- leaflet(tweet_locations) %>%
addTiles() %>%
addCircleMarkers(lng = ~long, lat = ~lat, popup = ~tweet_text,
radius = 3, stroke = FALSE)
site_locations
getwd()
json_file <- "C:/Users/miche/Documents/Twitter Analysis/boulder_flood_geolocated_tweets.json"
# import json file line by line to avoid syntax errors
# this takes a few seconds
boulder_flood_tweets <- stream_in(file(json_file))
tweet_data <- data.frame(date_time = boulder_flood_tweets$created_at,
username = boulder_flood_tweets$user$screen_name,
tweet_text = boulder_flood_tweets$text,
coords = boulder_flood_tweets$coordinates)
tweet_data
# flood start date sept 13 - 24 (end of incident)
start_date <- as.POSIXct('2013-09-13 00:00:00')
end_date <- as.POSIXct('2013-09-24 00:00:00')
# cleanup & and filter to just the time period around the flood
flood_tweets <- tweet_data %>%
mutate(coords = gsub("\\)|c\\(", "", coords),
date_time = as.POSIXct(date_time, format = "%a %b %d %H:%M:%S +0000 %Y")) %>%
separate(coords, c("long", "lat"), sep = ", ") %>%
mutate_at(c("lat", "long"), as.numeric) %>%
filter(date_time >= start_date & date_time <= end_date )
# cleanup & and filter to just the time period around the flood
flood_tweets <- tweet_data %>%
mutate(coords.coordinates = gsub("\\)|c\\(", "", coords.coordinates),
date_time = as.POSIXct(date_time, format = "%a %b %d %H:%M:%S +0000 %Y")) %>%
separate(coords.coordinates, c("long", "lat"), sep = ", ") %>%
mutate_at(c("lat", "long"), as.numeric) %>%
filter(date_time >= start_date & date_time <= end_date )
world_basemap <- ggplot() +
borders("world", colour = "gray85", fill = "gray80") +
theme_map()
world_basemap
# remove na values
tweet_locations <- flood_tweets %>%
na.omit()
world_basemap +
geom_point(data = tweet_locations, aes(x = long, y = lat),
colour = 'purple', alpha = .5) +
scale_size_continuous(range = c(1, 8),
breaks = c(250, 500, 750, 1000)) +
labs(title = "Tweet Locations During the Boulder Flood Event")
site_locations <- leaflet(tweet_locations) %>%
addTiles() %>%
addCircleMarkers(lng = ~long, lat = ~lat, popup = ~tweet_text,
radius = 3, stroke = FALSE)
site_locations
tweet_locations
# remove na values
tweet_locations <- flood_tweets %>%
na.omit()
tweet_locations
flood_tweets
# cleanup & and filter to just the time period around the flood
flood_tweets <- tweet_data %>%
mutate(coords.coordinates = gsub("\\)|c\\(", "", coords.coordinates),
date_time = as.POSIXct(date_time, format = "%a %b %d %H:%M:%S +0000 %Y")) %>%
separate(coords.coordinates, c("long", "lat"), sep = ", ") %>%
mutate_at(c("lat", "long"), as.numeric) %>%
filter(date_time >= start_date & date_time <= end_date )
flood_tweets
tweet_data <- data.frame(date_time = boulder_flood_tweets$created_at,
username = boulder_flood_tweets$user$screen_name,
tweet_text = boulder_flood_tweets$text,
coords = boulder_flood_tweets$coordinates)
# flood start date sept 13 - 24 (end of incident)
start_date <- as.POSIXct('2013-09-13 00:00:00')
end_date <- as.POSIXct('2013-09-24 00:00:00')
# cleanup & and filter to just the time period around the flood
flood_tweets <- tweet_data %>%
mutate(coords.coordinates = gsub("\\)|c\\(", "", coords.coordinates),
date_time = as.POSIXct(date_time, format = "%a %b %d %H:%M:%S +0000 %Y")) %>%
separate(coords.coordinates, c("long", "lat"), sep = ", ") %>%
mutate_at(c("lat", "long"), as.numeric) %>%
filter(date_time >= start_date & date_time <= end_date )
flood_tweets
tweet_data <- data.frame(date_time = boulder_flood_tweets$created_at,
username = boulder_flood_tweets$user$screen_name,
tweet_text = boulder_flood_tweets$text,
coords = boulder_flood_tweets$coordinates)
# flood start date sept 13 - 24 (end of incident)
start_date <- as.POSIXct('2013-09-13 00:00:00')
end_date <- as.POSIXct('2020-09-24 00:00:00')
# cleanup & and filter to just the time period around the flood
flood_tweets <- tweet_data %>%
mutate(coords.coordinates = gsub("\\)|c\\(", "", coords.coordinates),
date_time = as.POSIXct(date_time, format = "%a %b %d %H:%M:%S +0000 %Y")) %>%
separate(coords.coordinates, c("long", "lat"), sep = ", ") %>%
mutate_at(c("lat", "long"), as.numeric) %>%
filter(date_time >= start_date & date_time <= end_date )
flood_tweets
tweet_data
date_time
# cleanup & and filter to just the time period around the flood
flood_tweets <- tweet_data %>%
mutate(coords.coordinates = gsub("\\)|c\\(", "", coords.coordinates),
date_time = as.POSIXct(date_time, format = "%a %b %d %H:%M:%S +0000 %Y")) %>%
separate(coords.coordinates, c("long", "lat"), sep = ", ") %>%
mutate_at(c("lat", "long"), as.numeric) %>%
filter(date_time >= start_date & date_time <= end_date )
date_time
ate_time = boulder_flood_tweets$created_at
date_time = boulder_flood_tweets$created_at
date_time
# cleanup & and filter to just the time period around the flood
flood_tweets <- tweet_data %>%
mutate(coords.coordinates = gsub("\\)|c\\(", "", coords.coordinates),
date_time = as.POSIXct(date_time, format = "%a %b %d %H:%M:%S +0000 %Y")) %>%
separate(coords.coordinates, c("long", "lat"), sep = ", ") %>%
mutate_at(c("lat", "long"), as.numeric) %>%
filter(date_time >= start_date & date_time <= end_date )
flood_tweets
date_time
coords.coordinates
tweet_data <- data.frame(date_time = boulder_flood_tweets$created_at,
username = boulder_flood_tweets$user$screen_name,
tweet_text = boulder_flood_tweets$text,
coords = boulder_flood_tweets$coordinates)
coords
coords = boulder_flood_tweets$coordinates
coords
tweet_text
tweet_text = boulder_flood_tweets$text
username = boulder_flood_tweets$user$screen_name
tweet_data <- data.frame(date_time = boulder_flood_tweets$created_at,
username = boulder_flood_tweets$user$screen_name,
tweet_text = boulder_flood_tweets$text,
coords = boulder_flood_tweets$coordinates)
# flood start date sept 13 - 24 (end of incident)
start_date <- as.POSIXct('2013-09-13 00:00:00')
end_date <- as.POSIXct('2013-09-24 00:00:00')
# cleanup & and filter to just the time period around the flood
flood_tweets <- tweet_data %>%
mutate(coords.coordinates = gsub("\\)|c\\(", "", coords.coordinates),
date_time = as.POSIXct(date_time, format = "%a %b %d %H:%M:%S +0000 %Y")) %>%
separate(coords.coordinates, c("long", "lat"), sep = ", ") %>%
mutate_at(c("lat", "long"), as.numeric) %>%
filter(date_time >= start_date & date_time <= end_date )
flood_tweets
tweet_data
coords.coordinates
coords = boulder_flood_tweets$coordinates
coords.coordinates
coords
coords.coordinates
coords$coordinates
# cleanup & and filter to just the time period around the flood
flood_tweets <- tweet_data %>%
mutate(coords$coordinates = gsub("\\)|c\\(", "", coords$coordinates),
# cleanup & and filter to just the time period around the flood
flood_tweets <- tweet_data %>%
mutate(coords.coordinates = gsub("\\)|c\\(", "", coords$coordinates),
date_time = as.POSIXct(date_time, format = "%a %b %d %H:%M:%S +0000 %Y")) %>%
separate(coords$coordinates, c("long", "lat"), sep = ", ") %>%
mutate_at(c("lat", "long"), as.numeric) %>%
filter(date_time >= start_date & date_time <= end_date )
world_basemap +
geom_point(data = tweet_data, aes(x = long, y = lat),
colour = 'purple', alpha = .5) +
scale_size_continuous(range = c(1, 8),
breaks = c(250, 500, 750, 1000)) +
labs(title = "Tweet Locations During the Boulder Flood Event")
# cleanup & and filter to just the time period around the flood
flood_tweets <- tweet_data %>%
separate(coords$coordinates, c("long", "lat"), sep = ", ") %>%
mutate_at(c("lat", "long"), as.numeric) %>%
filter(date_time >= start_date & date_time <= end_date )
# cleanup & and filter to just the time period around the flood
flood_tweets <- tweet_data %>%
separate(coords.coordinates, c("long", "lat"), sep = ", ") %>%
mutate_at(c("lat", "long"), as.numeric) %>%
filter(date_time >= start_date & date_time <= end_date )
world_basemap +
geom_point(data = flood_tweets, aes(x = long, y = lat),
colour = 'purple', alpha = .5) +
scale_size_continuous(range = c(1, 8),
breaks = c(250, 500, 750, 1000)) +
labs(title = "Tweet Locations During the Boulder Flood Event")
coords$coordinates
mutate(coords.coordinates = gsub("\\)|c\\(", "", coords.coordinates),
date_time = as.POSIXct(date_time, format = "%a %b %d %H:%M:%S +0000 %Y"))
mutate(coords.coordinates = gsub("\\)|c\\(", "", coords$coordinates),
date_time = as.POSIXct(date_time, format = "%a %b %d %H:%M:%S +0000 %Y"))
# cleanup & and filter to just the time period around the flood
flood_tweets <- tweet_data %>%
separate(coords.coordinates, c("long", "lat"), sep = ", ") %>%
mutate_at(c("lat", "long"), as.numeric) %>%
filter(date_time >= start_date & date_time <= end_date )
flood_tweets
# cleanup & and filter to just the time period around the flood
flood_tweets <- tweet_data %>%
separate(coords$coordinates, c("long", "lat"), sep = ", ") %>%
mutate_at(c("lat", "long"), as.numeric) %>%
filter(date_time >= start_date & date_time <= end_date )
separate(coords$coordinates, c("long", "lat"), sep = ", ")
mutate(coords.coordinates = gsub("\\)|c\\(", "", coords.coordinates),
date_time = as.POSIXct(date_time, format = "%a %b %d %H:%M:%S +0000 %Y"))
flood_tweets <- tweet_data %>%
mutate(coords.coordinates = gsub("\\)|c\\(", "", coords.coordinates),
date_time = as.POSIXct(date_time, format = "%a %b %d %H:%M:%S +0000 %Y"))
flood_tweets
flood_tweets <- tweet_data %>%
mutate(coords.coordinates = gsub("\\)|c\\(", "", coords.coordinates),
date_time = as.POSIXct(date_time, format = "%a %b %d %H:%M:%S +0000 %Y")) %>%
separate(coords.coordinates, c("long", "lat"), sep = ", ")
flood_tweets
flood_tweets <- tweet_data %>%
mutate(coords.coordinates = gsub("\\)|c\\(", "", coords.coordinates),
date_time = as.POSIXct(date_time, format = "%a %b %d %H:%M:%S +0000 %Y")) %>%
separate(coords.coordinates, c("long", "lat"), sep = ", ") %>%
mutate_at(c("lat", "long"), as.numeric)
flood_tweets
flood_tweets <- tweet_data %>%
mutate(coords.coordinates = gsub("\\)|c\\(", "", coords.coordinates),
date_time = as.POSIXct(date_time, format = "%a %b %d %H:%M:%S +0000 %Y")) %>%
separate(coords.coordinates, c("long", "lat"), sep = ", ") %>%
mutate_at(c("lat", "long"), as.numeric) %>%
filter(date_time >= start_date & date_time <= end_date )
flood_tweets
flood_tweets <- tweet_data %>%
mutate(coords.coordinates = gsub("\\)|c\\(", "", coords.coordinates),
date_time = as.POSIXct(date_time, format = "%a %b %d %H:%M:%S +0000 %Y")) %>%
separate(coords.coordinates, c("long", "lat"), sep = ", ") %>%
mutate_at(c("lat", "long"), as.numeric)
flood_tweets
world_basemap <- ggplot() +
borders("world", colour = "gray85", fill = "gray80") +
theme_map()
# remove na values
tweet_locations <- flood_tweets %>%
na.omit()
tweet_locations
# cleanup & and filter to just the time period around the flood
flood_tweets <- tweet_data %>%
mutate(coords.coordinates = gsub("\\)|c\\(", "", coords.coordinates),
date_time = as.POSIXct(date_time, format = "%a %b %d %H:%M:%S +0000 %Y")) %>%
separate(coords.coordinates, c("long", "lat"), sep = ", ") %>%
mutate_at(c("lat", "long"), as.numeric)
flood_tweets
world_basemap +
geom_point(data = flood_tweets, aes(x = long, y = lat),
colour = 'purple', alpha = .5) +
scale_size_continuous(range = c(1, 8),
breaks = c(250, 500, 750, 1000)) +
labs(title = "Tweet Locations During the Boulder Flood Event")
